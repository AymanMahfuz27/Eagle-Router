{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "705e27dd",
   "metadata": {},
   "source": [
    "### EAGLE Router: Efficient Training-Free Router for Multi-LLM Inference\n",
    "\n",
    "This notebook-style script implements the EAGLE router described in the paper \"Eagle: Efficient Training-Free Router for Multi-LLM Inference\". It uses the RepliQA dataset evaluating GPT-4o vs GPT-4o-mini and follows these core ideas:\n",
    "\n",
    "- **Global ability**: a model’s general strength, learned from pairwise outcomes across all queries via ELO updates.\n",
    "- **Local ability**: a model’s query-specific strength, estimated by running ELO updates on only the nearest neighbor queries (by embedding similarity) to the target query.\n",
    "- **Final decision**: combine the two with a convex weight: FinalScore(model) = P × Global(model) + (1 − P) × Local(query, model).\n",
    "- **Training-free updates**: ELO is simple, fast, and can be updated online as new feedback arrives.\n",
    "\n",
    "I implement:\n",
    "- Data loading and preprocessing of the `notdiamond/repliqa_gpt4o_gpt4omini_evals` dataset.\n",
    "- OpenAI embeddings (`text-embedding-3-large`) for similarity.\n",
    "- Global ELO and Local ELO (nearest neighbors N=20 per Appendix A).\n",
    "- Hyperparameter tuning of P (grid search), with K=32 and N=20 per Appendix A.\n",
    "- Evaluation on a hold-out test set for (P=0.5, K=32, N=20) and (P=P', K=32, N=20).\n",
    "\n",
    "Dataset split: 1000 train / 100 validation / 100 test, initial ratings = 100.0. We handle prediction ties deterministically: if combined scores are equal, pick the higher global rating; if still equal, pick lexicographically by model name.\n",
    "\n",
    "Notes:\n",
    "- I chose scikit-learn `NearestNeighbors` (cosine) for simplicity and portability.\n",
    "- Grid search for P\n",
    "- Explanations accompany each section to justify the design and connect to the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d7ac350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "def pip_install(pkg):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "# Core libs\n",
    "pip_install(\"pandas\")\n",
    "pip_install(\"numpy\")\n",
    "pip_install(\"scikit-learn\")\n",
    "pip_install(\"tqdm\")\n",
    "\n",
    "# HF datasets\n",
    "pip_install(\"datasets\")\n",
    "\n",
    "# OpenAI client\n",
    "pip_install(\"openai>=1.40.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9164a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import hashlib\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Environment / config\n",
    "OPENAI_API_KEY = \"Lol nice try\"\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\"Please set the OPENAI_API_KEY environment variable.\")\n",
    "\n",
    "DATASET_NAME = \"notdiamond/repliqa_gpt4o_gpt4omini_evals\"\n",
    "\n",
    "# Paper defaults (Appendix A)\n",
    "PAPER_P = 0.5\n",
    "PAPER_K = 32.0\n",
    "PAPER_N = 20\n",
    "\n",
    "# Split sizes\n",
    "TRAIN_SIZE = 1000\n",
    "VAL_SIZE = 100\n",
    "TEST_SIZE = 100\n",
    "\n",
    "# ELO init\n",
    "INIT_RATING = 100.0\n",
    "\n",
    "# Embeddings\n",
    "EMBED_MODEL = \"text-embedding-3-large\"\n",
    "EMBED_CACHE_PATH = \"emb_cache_text-embedding-3-large.json\"\n",
    "\n",
    "# Random state\n",
    "RANDOM_SEED = 42\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# Make client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Tie decision margin for evaluation: if |score_a - score_b| <= margin, treat as predicted tie\n",
    "TIE_MARGIN = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac179e",
   "metadata": {},
   "source": [
    "### Helper Utilities\n",
    "\n",
    "- Robust text and label extraction from the dataset.\n",
    "- Lightweight embedding cache to avoid re-embedding the same text across runs.\n",
    "- Model name normalization to collapse versions and avoid spelling variants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76dc286c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "TEXT_COLUMN_CANDIDATES = [\n",
    "    \"question\", \"query\", \"prompt\", \"instruction\", \"text\", \"input\", \"user_input\", \"task\"\n",
    "]\n",
    "\n",
    "MODEL_A_COL_CANDS = [\"model_a\", \"A_model\", \"left_model\", \"model_left\", \"model1\", \"candidate_a\"]\n",
    "MODEL_B_COL_CANDS = [\"model_b\", \"B_model\", \"right_model\", \"model_right\", \"model2\", \"candidate_b\"]\n",
    "\n",
    "WINNER_COL_CANDS = [\n",
    "    \"winner\", \"winner_model\", \"preferred_model\", \"better\", \"choice\", \"label\", \"preferred\"\n",
    "]\n",
    "# Side indicator options (if provided): \"A\", \"B\", \"TIE\" or boolean columns like \"a_wins\"\n",
    "A_WINS_BOOL_CANDS = [\"a_wins\", \"A_wins\", \"left_wins\"]\n",
    "B_WINS_BOOL_CANDS = [\"b_wins\", \"B_wins\", \"right_wins\"]\n",
    "TIE_COL_CANDS = [\"tie\", \"is_tie\", \"draw\"]\n",
    "\n",
    "# We expect exactly two models overall for this dataset (gpt-4o vs gpt-4o-mini).\n",
    "KNOWN_MODELS = [\"gpt-4o\", \"gpt-4o-mini\"]\n",
    "KNOWN_SUBSTRINGS = {\n",
    "    \"gpt-4o-mini\": [\"gpt-4o-mini\"],\n",
    "    \"gpt-4o\": [\"gpt-4o\"],  # 'gpt-4o' must be checked after mini to keep mini-specific mapping\n",
    "}\n",
    "\n",
    "def normalize_model_name(name: str) -> str:\n",
    "    if not isinstance(name, str):\n",
    "        return str(name)\n",
    "    low = name.strip().lower()\n",
    "    # Try to map substrings to canonical names\n",
    "    for canonical, substrs in KNOWN_SUBSTRINGS.items():\n",
    "        for s in substrs:\n",
    "            if s in low:\n",
    "                return canonical\n",
    "    # Fallback: return stripped lowercase for reproducibility\n",
    "    return low\n",
    "\n",
    "def find_first_column(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def detect_model_from_colname(colname: str) -> Optional[str]:\n",
    "    \"\"\"Infer canonical model name from a column name that contains per-model scores.\n",
    "\n",
    "    This dataset stores columns like:\n",
    "      - 'gpt-4o-mini-2024-07-18/score'\n",
    "      - 'gpt-4o-2024-08-06/score'\n",
    "    We map them to canonical names 'gpt-4o-mini' and 'gpt-4o'.\n",
    "    \"\"\"\n",
    "    low = colname.strip().lower()\n",
    "    if \"score\" not in low:\n",
    "        return None\n",
    "    # Order matters: check 'gpt-4o-mini' before 'gpt-4o'\n",
    "    if \"gpt-4o-mini\" in low:\n",
    "        return \"gpt-4o-mini\"\n",
    "    if \"gpt-4o\" in low:\n",
    "        return \"gpt-4o\"\n",
    "    return None\n",
    "\n",
    "def find_model_score_columns(df: pd.DataFrame) -> Dict[str, str]:\n",
    "    \"\"\"Return mapping {canonical_model_name: score_column_name} if detectable.\"\"\"\n",
    "    mapping: Dict[str, str] = {}\n",
    "    for c in df.columns:\n",
    "        m = detect_model_from_colname(c)\n",
    "        if m is not None and m not in mapping:\n",
    "            mapping[m] = c\n",
    "    return mapping\n",
    "\n",
    "def infer_text_column(df: pd.DataFrame) -> str:\n",
    "    col = find_first_column(df, TEXT_COLUMN_CANDIDATES)\n",
    "    if col is None:\n",
    "        raise ValueError(f\"Could not infer text column. Available columns: {list(df.columns)}\")\n",
    "    return col\n",
    "\n",
    "def safe_str(x: Any) -> str:\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    return str(x)\n",
    "\n",
    "def infer_models_from_winner_values(df: pd.DataFrame, winner_col: str) -> List[str]:\n",
    "    vals = set()\n",
    "    for v in df[winner_col].dropna().unique().tolist():\n",
    "        vs = normalize_model_name(safe_str(v))\n",
    "        vals.add(vs)\n",
    "    # Keep only known ones if present\n",
    "    keep = [m for m in KNOWN_MODELS if m in vals]\n",
    "    if len(keep) >= 1:\n",
    "        # If only one appears, the other likely exists implicitly\n",
    "        if len(keep) == 1:\n",
    "            other = [m for m in KNOWN_MODELS if m != keep[0]]\n",
    "            keep = keep + other\n",
    "        return keep\n",
    "    # Fallback: return canonical two models\n",
    "    return KNOWN_MODELS[:]\n",
    "\n",
    "def parse_outcome_from_row(\n",
    "    row: pd.Series,\n",
    "    a_col: Optional[str],\n",
    "    b_col: Optional[str],\n",
    "    winner_col: Optional[str],\n",
    ") -> Tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Returns: (model_a, model_b, outcome_side)\n",
    "    outcome_side in {\"A\",\"B\",\"TIE\"}.\n",
    "    Heuristics cover:\n",
    "      - explicit model_a/model_b + winner (\"A\"/\"B\"/tie or model name)\n",
    "      - only winner model name, assume (gpt-4o, gpt-4o-mini) pair\n",
    "      - boolean a_wins/b_wins or tie columns\n",
    "    \"\"\"\n",
    "    # Models\n",
    "    if a_col and b_col:\n",
    "        model_a = normalize_model_name(row[a_col])\n",
    "        model_b = normalize_model_name(row[b_col])\n",
    "    else:\n",
    "        model_a, model_b = KNOWN_MODELS[0], KNOWN_MODELS[1]  # enforce canonical order\n",
    "\n",
    "    # Outcome: priority: tie col -> winner side -> a_wins/b_wins\n",
    "    # Tie columns\n",
    "    for tc in TIE_COL_CANDS:\n",
    "        if tc in row and pd.notna(row[tc]):\n",
    "            val = str(row[tc]).strip().lower()\n",
    "            if val in {\"1\", \"true\", \"yes\"} or val == \"tie\" or val == \"draw\":\n",
    "                return model_a, model_b, \"TIE\"\n",
    "\n",
    "    # Winner col\n",
    "    if winner_col and (winner_col in row):\n",
    "        w = row[winner_col]\n",
    "        if pd.isna(w):\n",
    "            # fallback to booleans\n",
    "            pass\n",
    "        else:\n",
    "            w_str = normalize_model_name(safe_str(w))\n",
    "            if w_str in {\"a\", \"left\"}:\n",
    "                return model_a, model_b, \"A\"\n",
    "            if w_str in {\"b\", \"right\"}:\n",
    "                return model_a, model_b, \"B\"\n",
    "            if w_str in {\"tie\", \"draw\"}:\n",
    "                return model_a, model_b, \"TIE\"\n",
    "            # Otherwise, assume it's a model name\n",
    "            if w_str == model_a:\n",
    "                return model_a, model_b, \"A\"\n",
    "            if w_str == model_b:\n",
    "                return model_a, model_b, \"B\"\n",
    "            # If winner uses only known canonical names, map accordingly\n",
    "            if w_str in KNOWN_MODELS:\n",
    "                if w_str == \"gpt-4o\":\n",
    "                    return model_a, model_b, (\"A\" if model_a == \"gpt-4o\" else \"B\")\n",
    "                if w_str == \"gpt-4o-mini\":\n",
    "                    return model_a, model_b, (\"A\" if model_a == \"gpt-4o-mini\" else \"B\")\n",
    "\n",
    "    # Boolean a_wins / b_wins\n",
    "    for ac in A_WINS_BOOL_CANDS:\n",
    "        if ac in row and pd.notna(row[ac]):\n",
    "            aval = str(row[ac]).strip().lower()\n",
    "            if aval in {\"1\", \"true\", \"yes\"}:\n",
    "                return model_a, model_b, \"A\"\n",
    "    for bc in B_WINS_BOOL_CANDS:\n",
    "        if bc in row and pd.notna(row[bc]):\n",
    "            bval = str(row[bc]).strip().lower()\n",
    "            if bval in {\"1\", \"true\", \"yes\"}:\n",
    "                return model_a, model_b, \"B\"\n",
    "\n",
    "    # As last fallback, if winner_col exists but ambiguous or missing,\n",
    "    # assume (gpt-4o vs gpt-4o-mini) and randomly assign no-winner as tie\n",
    "    return model_a, model_b, \"TIE\"\n",
    "\n",
    "def load_and_prepare_dataframe(dataset_name: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        ds = load_dataset(dataset_name, split=\"train\")\n",
    "    except Exception:\n",
    "        ds = load_dataset(dataset_name)\n",
    "        # pick the first available split\n",
    "        first_split = list(ds.keys())[0]\n",
    "        ds = ds[first_split]\n",
    "    df = ds.to_pandas()\n",
    "    # Ensure we have at least some rows\n",
    "    if len(df) == 0:\n",
    "        raise ValueError(\"Dataset loaded but empty.\")\n",
    "    return df\n",
    "\n",
    "def extract_matches(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts a normalized pairwise matches DataFrame with columns:\n",
    "      - 'text': query/prompt text\n",
    "      - 'model_a': canonical name\n",
    "      - 'model_b': canonical name\n",
    "      - 'outcome_side': \"A\" | \"B\" | \"TIE\"\n",
    "    \"\"\"\n",
    "    text_col = infer_text_column(df)\n",
    "    a_col = find_first_column(df, MODEL_A_COL_CANDS)\n",
    "    b_col = find_first_column(df, MODEL_B_COL_CANDS)\n",
    "    winner_col = find_first_column(df, WINNER_COL_CANDS)\n",
    "\n",
    "    # First, try to detect dataset-specific per-model score columns\n",
    "    score_cols = find_model_score_columns(df)\n",
    "    has_scores_for_both = all(m in score_cols for m in [\"gpt-4o\", \"gpt-4o-mini\"])\n",
    "\n",
    "    # If models not explicit and no score columns, try to infer from winner values\n",
    "    models_inferred = None\n",
    "    if (a_col is None or b_col is None) and not has_scores_for_both:\n",
    "        if winner_col:\n",
    "            models_inferred = infer_models_from_winner_values(df, winner_col)\n",
    "            if len(models_inferred) == 2:\n",
    "                # Set canonical order\n",
    "                global KNOWN_MODELS\n",
    "                KNOWN_MODELS = [models_inferred[0], models_inferred[1]]\n",
    "\n",
    "    records = []\n",
    "    # Prefer concatenating 'prompt' + 'question' when both exist for richer semantics\n",
    "    has_prompt = \"prompt\" in df.columns\n",
    "    has_question = \"question\" in df.columns\n",
    "    if has_scores_for_both:\n",
    "        # Build matches by comparing the two per-model score columns row-wise\n",
    "        col_gpt4o = score_cols[\"gpt-4o\"]\n",
    "        col_gpt4omini = score_cols[\"gpt-4o-mini\"]\n",
    "        for _, row in df.iterrows():\n",
    "            # Coerce to numeric; drop rows with missing scores\n",
    "            sa = pd.to_numeric(row[col_gpt4o], errors=\"coerce\")\n",
    "            sb = pd.to_numeric(row[col_gpt4omini], errors=\"coerce\")\n",
    "            if pd.isna(sa) or pd.isna(sb):\n",
    "                continue\n",
    "            if sa > sb:\n",
    "                side = \"A\"\n",
    "            elif sb > sa:\n",
    "                side = \"B\"\n",
    "            else:\n",
    "                side = \"TIE\"\n",
    "            # Enforce canonical order: A = gpt-4o, B = gpt-4o-mini\n",
    "            if has_prompt and has_question:\n",
    "                text_val = f\"{safe_str(row['prompt'])}\\n\\n{safe_str(row['question'])}\"\n",
    "            else:\n",
    "                text_val = safe_str(row[text_col])\n",
    "            records.append(\n",
    "                {\n",
    "                    \"text\": text_val,\n",
    "                    \"model_a\": \"gpt-4o\",\n",
    "                    \"model_b\": \"gpt-4o-mini\",\n",
    "                    \"outcome_side\": side,\n",
    "                    \"score_a\": int(sa),\n",
    "                    \"score_b\": int(sb),\n",
    "                }\n",
    "            )\n",
    "        return pd.DataFrame(records)\n",
    "    else:\n",
    "        # Fall back to generic parsing logic (A/B columns or winner column)\n",
    "        for _, row in df.iterrows():\n",
    "            if has_prompt and has_question:\n",
    "                text = f\"{safe_str(row['prompt'])}\\n\\n{safe_str(row['question'])}\"\n",
    "            else:\n",
    "                text = row[text_col]\n",
    "            model_a, model_b, outcome_side = parse_outcome_from_row(row, a_col, b_col, winner_col)\n",
    "            records.append(\n",
    "                {\n",
    "                    \"text\": safe_str(text),\n",
    "                    \"model_a\": model_a,\n",
    "                    \"model_b\": model_b,\n",
    "                    \"outcome_side\": outcome_side,\n",
    "                    \"score_a\": None,\n",
    "                    \"score_b\": None,\n",
    "                }\n",
    "            )\n",
    "        return pd.DataFrame(records)\n",
    "\n",
    "def sha256_text(s: str) -> str:\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def load_embedding_cache(path: str) -> Dict[str, List[float]]:\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_embedding_cache(path: str, cache: Dict[str, List[float]]) -> None:\n",
    "    tmp = path + \".tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cache, f)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Embeds a list of texts using OpenAI's text-embedding-3-large model.\n",
    "    Uses a simple on-disk cache to avoid recomputation across runs.\n",
    "    \"\"\"\n",
    "    cache = load_embedding_cache(EMBED_CACHE_PATH)\n",
    "    results: List[List[float]] = []\n",
    "    to_query: List[Tuple[int, str, str]] = []  # (idx, text, key)\n",
    "    for i, t in enumerate(texts):\n",
    "        key = sha256_text(t)\n",
    "        if key in cache:\n",
    "            results.append(cache[key])\n",
    "        else:\n",
    "            results.append(None)  # placeholder\n",
    "            to_query.append((i, t, key))\n",
    "\n",
    "    # Batch in chunks\n",
    "    BATCH = 64\n",
    "    for start in tqdm(range(0, len(to_query), BATCH), desc=\"Embedding\", unit=\"batch\"):\n",
    "        chunk = to_query[start:start + BATCH]\n",
    "        inputs = [t for _, t, _ in chunk]\n",
    "        resp = client.embeddings.create(model=EMBED_MODEL, input=inputs)\n",
    "        vecs = [d.embedding for d in resp.data]\n",
    "        for (i, _, key), v in zip(chunk, vecs):\n",
    "            results[i] = v\n",
    "            cache[key] = v\n",
    "        save_embedding_cache(EMBED_CACHE_PATH, cache)\n",
    "\n",
    "    # Type: List[List[float]]\n",
    "    return results  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aa129c",
   "metadata": {},
   "source": [
    "### ELO Implementation (Global and Local)\n",
    "\n",
    "- We use standard ELO with expected score E = 1 / (1 + 10^((Rb − Ra) / 400)).\n",
    "- Ratings update: R' = R + K × (S − E), with K=32 per the paper.\n",
    "- Global ELO: run updates over all train matches in random order.\n",
    "- Local ELO: for a target query, limit updates to the top-N nearest neighbor queries (by cosine similarity) from the training set.\n",
    "\n",
    "Handling ties:\n",
    "- For ties in outcomes, use S=0.5 for both sides.\n",
    "- For prediction ties (equal combined scores), pick the model with higher global rating, otherwise lexicographically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c071e5be",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def elo_expected(rating_a: float, rating_b: float) -> float:\n",
    "    \"\"\"Expected score for A vs B in ELO.\"\"\"\n",
    "    return 1.0 / (1.0 + 10.0 ** ((rating_b - rating_a) / 400.0))\n",
    "\n",
    "def elo_update(rating_a: float, rating_b: float, outcome_side: str, K: float) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Apply one ELO update for models A and B.\n",
    "    outcome_side: \"A\", \"B\", or \"TIE\".\n",
    "    \"\"\"\n",
    "    expected_a = elo_expected(rating_a, rating_b)\n",
    "    if outcome_side == \"A\":\n",
    "        score_a = 1.0\n",
    "    elif outcome_side == \"B\":\n",
    "        score_a = 0.0\n",
    "    else:\n",
    "        score_a = 0.5\n",
    "    score_b = 1.0 - score_a\n",
    "    new_a = rating_a + K * (score_a - expected_a)\n",
    "    new_b = rating_b + K * (score_b - (1.0 - expected_a))\n",
    "    return new_a, new_b\n",
    "\n",
    "def compute_global_ratings(\n",
    "    matches: List[Dict[str, Any]],\n",
    "    K: float = PAPER_K,\n",
    "    init_rating: float = INIT_RATING,\n",
    "    random_state: Optional[int] = RANDOM_SEED,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute global ELO ratings across all training matches.\n",
    "    Each match: {\"model_a\", \"model_b\", \"outcome_side\"}.\n",
    "    \"\"\"\n",
    "    ratings: Dict[str, float] = {}\n",
    "    for m in matches:\n",
    "        ratings.setdefault(m[\"model_a\"], init_rating)\n",
    "        ratings.setdefault(m[\"model_b\"], init_rating)\n",
    "\n",
    "    order = list(range(len(matches)))\n",
    "    rs = np.random.RandomState(random_state)\n",
    "    rs.shuffle(order)\n",
    "\n",
    "    for idx in order:\n",
    "        m = matches[idx]\n",
    "        a, b, side = m[\"model_a\"], m[\"model_b\"], m[\"outcome_side\"]\n",
    "        ra, rb = ratings[a], ratings[b]\n",
    "        na, nb = elo_update(ra, rb, side, K)\n",
    "        ratings[a], ratings[b] = na, nb\n",
    "    return ratings\n",
    "\n",
    "class LocalEloComputer:\n",
    "    \"\"\"\n",
    "    Computes local ELO ratings for a given query by running ELO updates\n",
    "    on the top-N nearest neighbor training matches.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_texts: List[str],\n",
    "        train_matches: List[Dict[str, Any]],\n",
    "        train_embeddings: np.ndarray,\n",
    "        K: float = PAPER_K,\n",
    "        init_rating: float = INIT_RATING,\n",
    "        N: int = PAPER_N,\n",
    "    ):\n",
    "        self.train_texts = train_texts\n",
    "        self.train_matches = train_matches\n",
    "        self.train_embeddings = train_embeddings\n",
    "        self.K = K\n",
    "        self.init_rating = init_rating\n",
    "        self.N = N\n",
    "\n",
    "        # Build index for neighbor lookup over train queries\n",
    "        self.nn = NearestNeighbors(\n",
    "            n_neighbors=min(N, len(train_embeddings)),\n",
    "            metric=\"cosine\",\n",
    "            algorithm=\"auto\",\n",
    "        )\n",
    "        self.nn.fit(train_embeddings)\n",
    "\n",
    "    def compute_local_for_query(self, query_vec: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Returns dict {model_name: local_rating} computed from neighbor matches.\n",
    "        \"\"\"\n",
    "        # Find neighbor indices\n",
    "        n_neighbors = min(self.N, len(self.train_embeddings))\n",
    "        distances, indices = self.nn.kneighbors(query_vec.reshape(1, -1), n_neighbors=n_neighbors)\n",
    "        neighbor_idxs = indices[0].tolist()\n",
    "\n",
    "        # Local ratings start at init\n",
    "        local: Dict[str, float] = {}\n",
    "        # Process neighbor matches in order of increasing distance (closer first)\n",
    "        # Optionally, could weight by distance; EAGLE uses pure neighbor set with ELO updates.\n",
    "        for idx in neighbor_idxs:\n",
    "            m = self.train_matches[idx]\n",
    "            a, b, side = m[\"model_a\"], m[\"model_b\"], m[\"outcome_side\"]\n",
    "            ra = local.get(a, self.init_rating)\n",
    "            rb = local.get(b, self.init_rating)\n",
    "            na, nb = elo_update(ra, rb, side, self.K)\n",
    "            local[a], local[b] = na, nb\n",
    "        return local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f868815",
   "metadata": {},
   "source": [
    "### Data Loading, Splitting, and Embeddings\n",
    "\n",
    "Steps:\n",
    "- Load the dataset from Hugging Face.\n",
    "- Extract pairwise matches with robust parsing.\n",
    "- Shuffle and split into 1000 train / 100 val / 100 test (or as many as available).\n",
    "- Compute OpenAI embeddings for all unique queries; build a nearest-neighbor index over train.\n",
    "\n",
    "Rationale:\n",
    "- The local signal depends on similarity between the current query and past queries, so we embed text once and reuse across splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e52dc452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes -> train: 1000, val: 100, test: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 19/19 [03:16<00:00, 10.32s/batch]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "raw_df = load_and_prepare_dataframe(DATASET_NAME)\n",
    "norm_df = extract_matches(raw_df)\n",
    "\n",
    "# Sanity check on models\n",
    "all_models = sorted(set(norm_df[\"model_a\"]).union(set(norm_df[\"model_b\"])))\n",
    "if set(KNOWN_MODELS).issubset(set(all_models)):\n",
    "    candidate_models = KNOWN_MODELS[:]\n",
    "else:\n",
    "    candidate_models = all_models\n",
    "\n",
    "# Shuffle and split\n",
    "perm = rng.permutation(len(norm_df))\n",
    "df_shuffled = norm_df.iloc[perm].reset_index(drop=True)\n",
    "\n",
    "target_train = min(TRAIN_SIZE, len(df_shuffled))\n",
    "target_val = min(VAL_SIZE, max(0, len(df_shuffled) - target_train))\n",
    "target_test = min(TEST_SIZE, max(0, len(df_shuffled) - target_train - target_val))\n",
    "\n",
    "train_df = df_shuffled.iloc[:target_train].reset_index(drop=True)\n",
    "val_df = df_shuffled.iloc[target_train:target_train + target_val].reset_index(drop=True)\n",
    "test_df = df_shuffled.iloc[target_train + target_val:target_train + target_val + target_test].reset_index(drop=True)\n",
    "\n",
    "print(f\"Split sizes -> train: {len(train_df)}, val: {len(val_df)}, test: {len(test_df)}\")\n",
    "\n",
    "# Prepare matches lists\n",
    "def df_to_matches(df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "    out = []\n",
    "    for _, r in df.iterrows():\n",
    "        out.append(\n",
    "            {\n",
    "                \"text\": r[\"text\"],\n",
    "                \"model_a\": r[\"model_a\"],\n",
    "                \"model_b\": r[\"model_b\"],\n",
    "                \"outcome_side\": r[\"outcome_side\"],\n",
    "            }\n",
    "        )\n",
    "    return out\n",
    "\n",
    "train_matches = df_to_matches(train_df)\n",
    "val_matches = df_to_matches(val_df)\n",
    "test_matches = df_to_matches(test_df)\n",
    "\n",
    "# Unique texts and embeddings\n",
    "all_texts = list(pd.unique(pd.concat([train_df[\"text\"], val_df[\"text\"], test_df[\"text\"]], ignore_index=True)))\n",
    "text_to_idx = {t: i for i, t in enumerate(all_texts)}\n",
    "all_emb_list = embed_texts(all_texts)\n",
    "all_emb = np.array(all_emb_list, dtype=np.float32)\n",
    "\n",
    "# Views\n",
    "def texts_to_vectors(texts: List[str]) -> np.ndarray:\n",
    "    idxs = [text_to_idx[t] for t in texts]\n",
    "    return all_emb[idxs, :]\n",
    "\n",
    "train_texts = train_df[\"text\"].tolist()\n",
    "val_texts = val_df[\"text\"].tolist()\n",
    "test_texts = test_df[\"text\"].tolist()\n",
    "\n",
    "train_vecs = texts_to_vectors(train_texts)\n",
    "val_vecs = texts_to_vectors(val_texts)\n",
    "test_vecs = texts_to_vectors(test_texts)\n",
    "\n",
    "# Local ELO computer (built over train only)\n",
    "local_computer = LocalEloComputer(\n",
    "    train_texts=train_texts,\n",
    "    train_matches=train_matches,\n",
    "    train_embeddings=train_vecs,\n",
    "    K=PAPER_K,\n",
    "    init_rating=INIT_RATING,\n",
    "    N=PAPER_N,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d371f5",
   "metadata": {},
   "source": [
    "### Scoring and Evaluation\n",
    "\n",
    "- Global ratings are computed once on train.\n",
    "- Local ratings are computed per query using the N nearest training neighbors.\n",
    "- Combined scores per model: P × Global + (1 − P) × Local.\n",
    "- Hyperparameter search for P on validation set.\n",
    "- Final evaluation on test set for P=0.5 and P=P'.\n",
    "\n",
    "Why grid search for P?\n",
    "- EAGLE is training-free; the only tradeoff is how much to rely on general vs local ability.\n",
    "- A small grid is easy to compute and interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a909eea7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "EPS = 1e-9\n",
    "\n",
    "def combined_scores_for_query(\n",
    "    models: List[str],\n",
    "    global_ratings: Dict[str, float],\n",
    "    local_ratings: Dict[str, float],\n",
    "    P: float,\n",
    ") -> Dict[str, float]:\n",
    "    out: Dict[str, float] = {}\n",
    "    for m in models:\n",
    "        g = global_ratings.get(m, INIT_RATING)\n",
    "        l = local_ratings.get(m, INIT_RATING)\n",
    "        out[m] = P * g + (1.0 - P) * l\n",
    "    return out\n",
    "\n",
    "def predict_for_query(\n",
    "    models: List[str],\n",
    "    query_vec: np.ndarray,\n",
    "    global_ratings: Dict[str, float],\n",
    "    local_computer: LocalEloComputer,\n",
    "    P: float,\n",
    ") -> str:\n",
    "    local_ratings = local_computer.compute_local_for_query(query_vec)\n",
    "    scores = combined_scores_for_query(models, global_ratings, local_ratings, P)\n",
    "\n",
    "    # Pick argmax with deterministic tie-breaks\n",
    "    m_sorted = sorted(models)\n",
    "    best_model = m_sorted[0]\n",
    "    best_score = scores[best_model]\n",
    "    for m in m_sorted[1:]:\n",
    "        sc = scores[m]\n",
    "        if sc > best_score + EPS:\n",
    "            best_score = sc\n",
    "            best_model = m\n",
    "        elif abs(sc - best_score) <= EPS:\n",
    "            # tie -> pick higher global rating\n",
    "            g_best = global_ratings.get(best_model, INIT_RATING)\n",
    "            g_m = global_ratings.get(m, INIT_RATING)\n",
    "            if g_m > g_best + EPS:\n",
    "                best_model = m\n",
    "                best_score = sc\n",
    "            elif abs(g_m - g_best) <= EPS:\n",
    "                # final tie-break: lexicographic\n",
    "                if m < best_model:\n",
    "                    best_model = m\n",
    "                    best_score = sc\n",
    "    return best_model\n",
    "\n",
    "def outcome_to_winner_name(model_a: str, model_b: str, side: str) -> Optional[str]:\n",
    "    if side == \"A\":\n",
    "        return model_a\n",
    "    if side == \"B\":\n",
    "        return model_b\n",
    "    # TIE\n",
    "    return None\n",
    "\n",
    "def evaluate_matches(\n",
    "    matches: List[Dict[str, Any]],\n",
    "    vectors: np.ndarray,\n",
    "    global_ratings: Dict[str, float],\n",
    "    local_computer: LocalEloComputer,\n",
    "    P: float,\n",
    ") -> Tuple[float, List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Returns (routing_accuracy, detailed_results).\n",
    "    - If ground-truth is a tie (equal scores), count correct when our combined scores are effectively tied.\n",
    "    \"\"\"\n",
    "    correct = 0.0\n",
    "    detailed: List[Dict[str, Any]] = []\n",
    "\n",
    "    for i, m in enumerate(matches):\n",
    "        a, b, side = m[\"model_a\"], m[\"model_b\"], m[\"outcome_side\"]\n",
    "        models = [a, b]\n",
    "        pred = predict_for_query(models, vectors[i], global_ratings, local_computer, P)\n",
    "        winner = outcome_to_winner_name(a, b, side)\n",
    "\n",
    "        local_ratings = local_computer.compute_local_for_query(vectors[i])\n",
    "        scores = combined_scores_for_query(models, global_ratings, local_ratings, P)\n",
    "        is_pred_tie = abs(scores[a] - scores[b]) <= TIE_MARGIN\n",
    "\n",
    "        if winner is None:\n",
    "            # Treat dataset ties as correct if our combined scores tie\n",
    "            if is_pred_tie:\n",
    "                correct += 1.0\n",
    "        else:\n",
    "            if pred == winner:\n",
    "                correct += 1.0\n",
    "\n",
    "        detailed.append(\n",
    "            {\n",
    "                \"text\": m[\"text\"],\n",
    "                \"model_a\": a,\n",
    "                \"model_b\": b,\n",
    "                \"gt_winner\": winner if winner is not None else \"TIE\",\n",
    "                \"pred_winner\": pred if not is_pred_tie else \"TIE\",\n",
    "                \"score_a\": m.get(\"score_a\"),\n",
    "                \"score_b\": m.get(\"score_b\"),\n",
    "                \"comb_a\": scores[a],\n",
    "                \"comb_b\": scores[b],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    acc = correct / max(1, len(matches))\n",
    "    return acc, detailed\n",
    "\n",
    "def evaluate_matches_strict_non_tie(\n",
    "    matches: List[Dict[str, Any]],\n",
    "    vectors: np.ndarray,\n",
    "    global_ratings: Dict[str, float],\n",
    "    local_computer: LocalEloComputer,\n",
    "    P: float,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Strict accuracy on rows where the dataset has a non-tie winner.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, m in enumerate(matches):\n",
    "        a, b, side = m[\"model_a\"], m[\"model_b\"], m[\"outcome_side\"]\n",
    "        winner = outcome_to_winner_name(a, b, side)\n",
    "        if winner is None:\n",
    "            continue\n",
    "        total += 1\n",
    "        pred = predict_for_query([a, b], vectors[i], global_ratings, local_computer, P)\n",
    "        if pred == winner:\n",
    "            correct += 1\n",
    "    if total == 0:\n",
    "        return float(\"nan\")\n",
    "    return correct / total\n",
    "\n",
    "def grid_search_P(\n",
    "    P_values: List[float],\n",
    "    val_matches: List[Dict[str, Any]],\n",
    "    val_vecs: np.ndarray,\n",
    "    global_ratings: Dict[str, float],\n",
    "    local_computer: LocalEloComputer,\n",
    "    metric: str = \"non_tie\",\n",
    ") -> Tuple[float, List[Tuple[float, float]]]:\n",
    "    \"\"\"\n",
    "    Grid search for P. Metric options:\n",
    "      - \"non_tie\": strict accuracy on non-tie rows only (recommended for this dataset)\n",
    "      - \"routing\": tie-aware accuracy using evaluate_matches\n",
    "    Returns (best_p, curve[(P, acc)]) where acc is according to metric.\n",
    "    \"\"\"\n",
    "    results: List[Tuple[float, float]] = []\n",
    "    best_p = P_values[0]\n",
    "    best_acc = -1.0\n",
    "    for p in P_values:\n",
    "        if metric == \"routing\":\n",
    "            acc, _ = evaluate_matches(val_matches, val_vecs, global_ratings, local_computer, p)\n",
    "        else:\n",
    "            acc = evaluate_matches_strict_non_tie(val_matches, val_vecs, global_ratings, local_computer, p)\n",
    "            if np.isnan(acc):\n",
    "                acc = 0.0\n",
    "        results.append((p, acc))\n",
    "        if acc > best_acc + EPS or (abs(acc - best_acc) <= EPS and p < best_p):\n",
    "            best_acc = acc\n",
    "            best_p = p\n",
    "    return best_p, results\n",
    "\n",
    "def compute_tie_rate(matches: List[Dict[str, Any]]) -> float:\n",
    "    total = len(matches)\n",
    "    if total == 0:\n",
    "        return float(\"nan\")\n",
    "    ties = 0\n",
    "    for m in matches:\n",
    "        sa = m.get(\"score_a\")\n",
    "        sb = m.get(\"score_b\")\n",
    "        if sa is not None and sb is not None and sa == sb:\n",
    "            ties += 1\n",
    "    return ties / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d755f7",
   "metadata": {},
   "source": [
    "### Run: Global ELO, Grid Search P, and Final Evaluation\n",
    "\n",
    "1. Compute global ratings on train using K=32, init=100.\n",
    "2. Grid search P in [0.0, 1.0] with step 0.05 on validation.\n",
    "3. Evaluate test accuracy for paper params (P=0.5, K=32, N=20) and optimized P'.\n",
    "4. Print:\n",
    "   - Global ratings (sorted).\n",
    "   - Best P'.\n",
    "   - Validation accuracy curve (P vs accuracy).\n",
    "   - Test accuracies under both configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa7ebf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Global ELO Ratings (K=32, init=100) ===\n",
      "gpt-4o-mini     : 121.7346\n",
      "gpt-4o          : 78.2654\n",
      "\n",
      "=== Best P' from validation ===\n",
      "Best P' = 0.25\n",
      "\n",
      "=== Validation Strict (non-tie) Accuracy Curve (P vs Accuracy) ===\n",
      "P    | Val Strict Acc\n",
      "-----+---------------\n",
      "0.00 | 73.68%        \n",
      "0.05 | 73.68%        \n",
      "0.10 | 73.68%        \n",
      "0.15 | 73.68%        \n",
      "0.20 | 73.68%        \n",
      "0.25 | 78.95%        \n",
      "0.30 | 68.42%        \n",
      "0.35 | 68.42%        \n",
      "0.40 | 73.68%        \n",
      "0.45 | 73.68%        \n",
      "0.50 | 73.68%        \n",
      "0.55 | 73.68%        \n",
      "0.60 | 73.68%        \n",
      "0.65 | 73.68%        \n",
      "0.70 | 73.68%        \n",
      "0.75 | 73.68%        \n",
      "0.80 | 73.68%        \n",
      "0.85 | 73.68%        \n",
      "0.90 | 73.68%        \n",
      "0.95 | 73.68%        \n",
      "1.00 | 73.68%        \n",
      "\n",
      "=== Test Set Accuracy ===\n",
      "Paper params (P=0.50, K=32, N=20): 10.00%\n",
      "Optimized P' (P=0.25, K=32, N=20): 13.00%\n",
      "\n",
      "=== Tie diagnostics and Strict (non-tie) Accuracy ===\n",
      "Val tie rate (dataset): 0.00%\n",
      "Test tie rate (dataset): 0.00%\n",
      "Val routing acc (P=0.50): 15.00% | strict: 73.68%\n",
      "Val routing acc (P=0.25): 17.00% | strict: 78.95%\n",
      "Test routing acc (P=0.50): 10.00% | strict: 64.29%\n",
      "Test routing acc (P=0.25): 13.00% | strict: 71.43%\n"
     ]
    }
   ],
   "source": [
    "# 1) Global ratings on train\n",
    "global_ratings = compute_global_ratings(train_matches, K=PAPER_K, init_rating=INIT_RATING, random_state=RANDOM_SEED)\n",
    "\n",
    "# 2) Grid search P on validation (optimize strict non-tie accuracy by default)\n",
    "P_grid = [round(x, 2) for x in np.linspace(0.0, 1.0, 21)]\n",
    "best_p, val_curve = grid_search_P(P_grid, val_matches, val_vecs, global_ratings, local_computer, metric=\"non_tie\")\n",
    "\n",
    "# 3) Evaluate test under paper P and best P'\n",
    "paper_test_acc, _ = evaluate_matches(test_matches, test_vecs, global_ratings, local_computer, PAPER_P)\n",
    "bestp_test_acc, _ = evaluate_matches(test_matches, test_vecs, global_ratings, local_computer, best_p)\n",
    "paper_val_acc, _ = evaluate_matches(val_matches, val_vecs, global_ratings, local_computer, PAPER_P)\n",
    "bestp_val_acc, _ = evaluate_matches(val_matches, val_vecs, global_ratings, local_computer, best_p)\n",
    "\n",
    "# Strict non-tie accuracies (diagnostic)\n",
    "paper_val_acc_strict = evaluate_matches_strict_non_tie(val_matches, val_vecs, global_ratings, local_computer, PAPER_P)\n",
    "bestp_val_acc_strict = evaluate_matches_strict_non_tie(val_matches, val_vecs, global_ratings, local_computer, best_p)\n",
    "paper_test_acc_strict = evaluate_matches_strict_non_tie(test_matches, test_vecs, global_ratings, local_computer, PAPER_P)\n",
    "bestp_test_acc_strict = evaluate_matches_strict_non_tie(test_matches, test_vecs, global_ratings, local_computer, best_p)\n",
    "\n",
    "# 4) Print formatted results\n",
    "def format_table(rows: List[Tuple[Any, ...]], headers: List[str]) -> str:\n",
    "    col_widths = [max(len(str(h)), max((len(str(r[i])) for r in rows), default=0)) for i, h in enumerate(headers)]\n",
    "    header_line = \" | \".join(h.ljust(col_widths[i]) for i, h in enumerate(headers))\n",
    "    sep_line = \"-+-\".join(\"-\" * col_widths[i] for i in range(len(headers)))\n",
    "    body_lines = [\" | \".join(str(r[i]).ljust(col_widths[i]) for i in range(len(headers))) for r in rows]\n",
    "    return \"\\n\".join([header_line, sep_line] + body_lines)\n",
    "\n",
    "# Global ratings\n",
    "print(\"\\n=== Global ELO Ratings (K=32, init=100) ===\")\n",
    "gr_sorted = sorted(global_ratings.items(), key=lambda kv: kv[1], reverse=True)\n",
    "for m, s in gr_sorted:\n",
    "    print(f\"{m:16s}: {s:0.4f}\")\n",
    "\n",
    "# Best P'\n",
    "print(f\"\\n=== Best P' from validation ===\\nBest P' = {best_p:0.2f}\")\n",
    "\n",
    "# Validation accuracy curve\n",
    "print(\"\\n=== Validation Strict (non-tie) Accuracy Curve (P vs Accuracy) ===\")\n",
    "rows = [(f\"{p:0.2f}\", f\"{acc*100:0.2f}%\") for p, acc in val_curve]\n",
    "print(format_table(rows, headers=[\"P\", \"Val Strict Acc\"]))\n",
    "\n",
    "# Test accuracies\n",
    "print(\"\\n=== Test Set Accuracy ===\")\n",
    "print(f\"Paper params (P=0.50, K=32, N=20): {paper_test_acc*100:0.2f}%\")\n",
    "print(f\"Optimized P' (P={best_p:0.2f}, K=32, N=20): {bestp_test_acc*100:0.2f}%\")\n",
    "\n",
    "print(\"\\n=== Tie diagnostics and Strict (non-tie) Accuracy ===\")\n",
    "val_tie_rate = compute_tie_rate(val_matches)\n",
    "test_tie_rate = compute_tie_rate(test_matches)\n",
    "print(f\"Val tie rate (dataset): {val_tie_rate*100 if not np.isnan(val_tie_rate) else float('nan'):0.2f}%\")\n",
    "print(f\"Test tie rate (dataset): {test_tie_rate*100 if not np.isnan(test_tie_rate) else float('nan'):0.2f}%\")\n",
    "print(f\"Val routing acc (P=0.50): {paper_val_acc*100:0.2f}% | strict: {paper_val_acc_strict*100 if not np.isnan(paper_val_acc_strict) else float('nan'):0.2f}%\")\n",
    "print(f\"Val routing acc (P={best_p:0.2f}): {bestp_val_acc*100:0.2f}% | strict: {bestp_val_acc_strict*100 if not np.isnan(bestp_val_acc_strict) else float('nan'):0.2f}%\")\n",
    "print(f\"Test routing acc (P=0.50): {paper_test_acc*100:0.2f}% | strict: {paper_test_acc_strict*100 if not np.isnan(paper_test_acc_strict) else float('nan'):0.2f}%\")\n",
    "print(f\"Test routing acc (P={best_p:0.2f}): {bestp_test_acc*100:0.2f}% | strict: {bestp_test_acc_strict*100 if not np.isnan(bestp_test_acc_strict) else float('nan'):0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6ed6e1",
   "metadata": {},
   "source": [
    "### Summary and Interpretation \n",
    "\n",
    "- EAGLE decomposes ability into global (broad strength) and local (query-specific via nearest neighbors), both updated with ELO from pairwise feedback.\n",
    "- This implementation:\n",
    "  - Initializes all model ratings at 100.0.\n",
    "  - Uses K=32, N=20 (Appendix A).\n",
    "  - Combines scores as P × Global + (1 − P) × Local and tunes P on validation.\n",
    "- Ties:\n",
    "  - Match ties (no clear winner) update both models with S=0.5.\n",
    "  - Prediction ties are broken by global rating, then lexicographic order.\n",
    "- Efficiency:\n",
    "  - Embedding is batched and cached; local ELO uses only N neighbor updates per query.\n",
    "  - Updating with new feedback requires only re-running ELO updates on the new comparisons, keeping it training-free and fast.\n",
    "\n",
    "  \n",
    "\n",
    "To better understand the performance of the EAGLE router, I combined two perspectives:  \n",
    "(1) the raw dataset distribution from a SQL query, and  \n",
    "(2) routing vs. strict accuracy from my implementation.\n",
    "\n",
    "#### Dataset Imbalance\n",
    "A SQL query over the training split compared model scores row by row:\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN \"gpt-4o-mini-2024-07-18/score\" > \"gpt-4o-2024-08-06/score\" THEN 'gpt-4o-mini wins'\n",
    "        WHEN \"gpt-4o-2024-08-06/score\" > \"gpt-4o-mini-2024-07-18/score\" THEN 'gpt-4o wins'\n",
    "        ELSE 'tie'\n",
    "    END AS result,\n",
    "    COUNT(*) AS count\n",
    "FROM train\n",
    "GROUP BY result\n",
    "ORDER BY count DESC;\n",
    "```\n",
    "\n",
    "Results:\n",
    "\n",
    "* **gpt-4o-mini wins:** 8,327\n",
    "* **gpt-4o wins:** 538\n",
    "* **ties:** 1,198\n",
    "\n",
    "This confirms the dataset is heavily skewed toward **gpt-4o-mini** (≈83% of rows). Always predicting gpt-4o-mini would already achieve a strong baseline. This explains why routing accuracy alone appears deceptively low — the metric is dominated by dataset imbalance.\n",
    "\n",
    "#### Router Evaluation\n",
    "\n",
    "Despite this imbalance, the EAGLE router shows meaningful signal:\n",
    "\n",
    "* **Global ELO Ratings**: gpt-4o-mini (121.7) > gpt-4o (78.3)\n",
    "* **Optimal P'**: 0.25 (validation search)\n",
    "* **Validation (strict, non-tie)**: up to **78.95%** accuracy\n",
    "* **Test (strict, non-tie)**: \\~**71.43%** accuracy\n",
    "\n",
    "In contrast, raw routing accuracy is much lower (\\~10–17%) because it is diluted by the overwhelming number of “mini wins.”\n",
    "\n",
    "#### Takeaway\n",
    "\n",
    "* The dataset’s **imbalance** drives low routing accuracy.\n",
    "* On the **more informative strict non-tie subset**, the EAGLE router consistently achieves **\\~70% accuracy**, showing that it successfully leverages both global and local ELO signals to make meaningful distinctions.\n",
    "* This validates the paper’s core claim: even in imbalanced settings, combining global ability with query-specific local ability improves routing fidelity.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
